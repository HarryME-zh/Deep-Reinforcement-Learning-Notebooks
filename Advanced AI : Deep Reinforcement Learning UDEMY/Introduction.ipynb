{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDP)\n",
    "Reinforcement learning problems are usually representend with the MDP framework, meaning those:\n",
    "- Set of State\n",
    "- Set of actions\n",
    "- Set of rewards\n",
    "- Discount Factor ( Gamma )\n",
    "- State-transition probabilites ( because not all the environements are deterministic )\n",
    "\n",
    "\n",
    "One way of solving it is by using \n",
    "## Dynamic Programming :\n",
    "2 Iteratives functions way :\n",
    "1. Policy Evaluation :\n",
    "Bellman's Equation allow us to define the value function ( for a state ) recursively :\n",
    "\\begin{align}\n",
    "V_\\pi (s) = \\sum_a \\pi (a|s) \\sum_s' \\sum_r p(s', r|s, a){r+ \\gamma V_\\pi (s') }\n",
    "\\end{align}\n",
    "and because it is a linear equation, we can develop it and obtain a **policy evaluation**\n",
    "2. Policy Improvement ( take the argmax over Q(s, a))\n",
    "\n",
    "### Value Iteration\n",
    "- Instead of waiting for the policy evaluation to converge, just do 1 iteration\n",
    "- It will still converge\n",
    "- We don't need to do Policy Improvement step at all\n",
    "- Since it's argmax, then taking the max in the policy evaluation step is equivalent\n",
    "\\begin{align}\n",
    "    V_k+1 (s) = max_a \\sum_s' \\sum_r p(s', r|s, a){r + \\gamma V_k (s')}\n",
    "\\end{align}\n",
    "\n",
    "But Dynamic Programming requires us to know all of the transitions probabilites which is impossible in real life problems.\n",
    "Also we need to loop through all states on every iteration which is not practible on large domain of S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo (MC) :\n",
    "MC is all about learning from experience, where expected values can be approximated by sample means\n",
    "Play a bunch of episodes, gather returns, average them \n",
    "\\begin{align}\n",
    "        V(s) = E[G(t)|S(t)=s] \\approx \\frac{1}{N} \\sum_{i=1}^{N} G_{i,s}\n",
    "\\end{align}\n",
    "\n",
    "It gives us only Value function for the states that we met.\n",
    "And the problem is that :\n",
    "- it won't always work.\n",
    "- requires many episodes.\n",
    "- don't work on not episodic task.\n",
    "- the episode might never end if it bumps into a wall.\n",
    "- agent won't do it again, since policy choose argmax.\n",
    "- it can leave many states unexplored ( one possible solution is the \"random start\" or the \"epsilon-greedy\" method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning (TD) :\n",
    "- Unique to RL\n",
    "- MC: Sample returns based on an episode\n",
    "- TD : Estimate returns based on current value function estimate\n",
    "- We looked at TD(0).\n",
    "- Instead of using G (which is the sampled return ), we use $ r + \\gamma V(s') $ which estimates the return from the next state and onward r.\n",
    "- Its like calculating the mean ( which is space consuming because we need to store all of the samples episodes ) but in a more optimized way : we can calculate current sample mean from last sample mean.\n",
    "\\begin{align}\n",
    "V_t (s) = V_{t-1} (s) + frac{1}{t} [G_t - V_{t-1} (s)] = \\frac{1}{t} \\sum_{t'=1}^{t} G_{t'}\n",
    "\\end{align}\n",
    "\n",
    "Which can be generalized like that (looking like Gradient Descent) :\n",
    "\\begin{align}\n",
    "V(s) = V(s) + \\alpha [G - V(s)]\n",
    "\\end{align}\n",
    "\n",
    "**TD(0)**\n",
    "- Instead of using G, we us current reward r and the next state value V(s)\n",
    "- We only need to wait t+1 to update value for state at time t while MC need to wait the until the end of entire episode.\n",
    "\n",
    "**TD Control**\n",
    "\\begin{align}\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma max_{a'} Q(s',a') - Q(s,a)]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But those methos do now work on larger problems with billions of states and actions ( because here we use a dictionary of state/action ), that's why we will need approximation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Knowing that supervised-learning is used a function approximator\n",
    "- We want to estimate V(s) or Q(s, a) :\n",
    "\\begin{align}\n",
    "Input : x = \\phi (s), Target : G\n",
    "\\end{align}\n",
    "\n",
    "- Since reward is a real number, so is G, therefore we do regression, and the appropriate loss is a squared error \n",
    "\\begin{align}\n",
    "E = \\sum_{n=1}^{N} (G_n - V ( \\phi ( S_n); 0))^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we need a differentiable model so we can use gradient descent \n",
    "\\begin{align}\n",
    "V ( \\phi ( S_n);\\theta) = \\theta^T \\phi (S_n)\n",
    "\\end{align}\n",
    "\n",
    "- $\\theta$ is the model parameter, so that's what we need to update\n",
    "\\begin{align}\n",
    "\\theta = \\theta - \\alpha ( \\frac{\\delta E}{\\delta \\theta} ) = \\theta + \\alpha (G - V) \\phi(S)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
